{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Import Library*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memuat Data Ulasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"ulasan akhir.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "df['Ulasan'] = df['Ulasan'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Preprocessing*\n",
    "### *Cleaning* & *Casefolding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_casefolding(ulasan):\n",
    "    ulasan = ulasan.strip(\" \")\n",
    "    ulasan = re.sub(r'[?|#@$!_:\\\"\\'.,\\(\\)\\[\\]\\{\\}\\+\\-\\/\\*\\^\\%\\=\\<\\>\\&\\~\\`\\;0-9]', ' ', ulasan)\n",
    "    ulasan = re.sub(r'[^\\w\\s]', ' ', ulasan)\n",
    "    ulasan = re.sub(r\"\\b\\w\\b\", \"\", ulasan)\n",
    "    ulasan = \" \".join(ulasan.split())\n",
    "    ulasan = ulasan.lower()\n",
    "    return ulasan\n",
    "df['cleaning_casefolding'] = df['Ulasan'].apply(cleaning_casefolding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_slangwords = \"kamus alay.csv\"\n",
    "slangwords = pd.read_csv(file_slangwords, encoding='utf-8')\n",
    "\n",
    "def normalisasi(ulasan):\n",
    "    for j, f in zip(slangwords['slang'], slangwords['formal']):\n",
    "        f = str(f) \n",
    "        ulasan = re.sub(rf\"\\b{j}\\b\", f, ulasan) \n",
    "    return ulasan\n",
    "df['normalisasi'] = df['cleaning_casefolding'].apply(normalisasi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Convert Negation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_negation(ulasan):\n",
    "    kata_negasi = [\"tidak\", \"tiada\", \"belum\", \"jangan\", \"tanpa\",\"bukan\"]\n",
    "    for negasi in kata_negasi:\n",
    "        ulasan= re.sub(rf\"\\b({negasi})\\b (\\w+)\", rf\"\\1 \\2_neg\", ulasan)\n",
    "    return ulasan\n",
    "df['convert_negation'] = df['normalisasi'].apply(convert_negation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tokenizing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(ulasan):\n",
    "    list_token = ulasan.split(' ')\n",
    "    list_token = [token for token in list_token if token != '']\n",
    "    return list_token\n",
    "df['tokenizing'] = df['convert_negation'].apply(tokenizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Stopword Removal*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(tokens):\n",
    "    custom_stopwords = pd.read_excel('stopwords-indonesia.xlsx')['stopwords'].tolist()\n",
    "    tokens_without_stopwords = [token for token in tokens if token not in custom_stopwords]\n",
    "    return tokens_without_stopwords\n",
    "df['stopword_removal'] = df['tokenizing'].apply(stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Stemming*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_nazief_andriani(tokens):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "        parts = token.split('_')\n",
    "        stemmed_parts = [stemmer.stem(part) for part in parts]\n",
    "        stemmed_token = '_'.join(stemmed_parts)\n",
    "        stemmed_tokens.append(stemmed_token)\n",
    "    return ' '.join(stemmed_tokens)\n",
    "df['stemming'] = df['stopword_removal'].apply(stemming_nazief_andriani)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Tranformation*\n",
    "### Pembagian Data 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['stemming'].values\n",
    "y = df['Sentimen'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22, stratify=y)\n",
    "\n",
    "train_df = pd.DataFrame({'stemming': X_train, 'Sentimen': y_train})\n",
    "test_df = pd.DataFrame({'stemming': X_test, 'Sentimen': y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembobotan *Term* (TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "normalized_tfidf_matrix_train = normalize(tfidf_matrix_train, norm='l2')\n",
    "\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "normalized_tfidf_matrix_test = normalize(tfidf_matrix_test, norm='l2')\n",
    "\n",
    "df_tfidf_train = pd.DataFrame(normalized_tfidf_matrix_train.toarray())\n",
    "df_tfidf_test = pd.DataFrame(normalized_tfidf_matrix_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Data Mining*\n",
    "### Naive Bayes Tanpa SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(df_tfidf_train, train_df['Sentimen'])\n",
    "y_pred = naive_bayes_classifier.predict(df_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_naive_bayes = pd.DataFrame({\n",
    "    'Ulasan': test_df['stemming'],  \n",
    "    'Label_Sebenarnya': test_df['Sentimen'],  \n",
    "    'Prediksi': y_pred\n",
    "})\n",
    "output_naive_bayes.to_excel(\"output_naive_bayes.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Menggunakan SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(df_tfidf_train, train_df['Sentimen'])\n",
    "naive_bayes_classifier_smote = MultinomialNB()\n",
    "naive_bayes_classifier_smote.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_smote = naive_bayes_classifier_smote.predict(df_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_naive_bayes_smote = pd.DataFrame({\n",
    "    'Ulasan': test_df['stemming'],  \n",
    "    'Label_Sebenarnya': test_df['Sentimen'],  \n",
    "    'Prediksi': y_pred_smote \n",
    "})\n",
    "output_naive_bayes_smote.to_excel(\"output_naive_bayes_smote.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Interpretation/Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Data Training =====\n",
      "negatif: 244\n",
      "netral: 115\n",
      "positif: 1063\n",
      "\n",
      "===== Data Testing =====\n",
      "negatif: 61\n",
      "netral: 29\n",
      "positif: 266\n",
      "\n",
      "Akurasi tanpa menggunakan SMOTE: 0.83\n",
      "Kelas negatif: Presisi = 0.91, Recall = 0.52\n",
      "Kelas netral: Presisi = 0.00, Recall = 0.00\n",
      "Kelas positif: Presisi = 0.83, Recall = 1.00\n",
      "\n",
      "Confusion Matrix tanpa menggunakan SMOTE:\n",
      "[[ 32   0  29]\n",
      " [  2   0  27]\n",
      " [  1   0 265]]\n",
      "\n",
      "Akurasi menggunakan SMOTE: 0.84\n",
      "Kelas negatif setelah SMOTE: Presisi = 0.79, Recall = 0.75\n",
      "Kelas netral setelah SMOTE: Presisi = 0.29, Recall = 0.52\n",
      "Kelas positif setelah SMOTE: Presisi = 0.97, Recall = 0.89\n",
      "\n",
      "Confusion Matrix menggunakan SMOTE:\n",
      "[[ 46  14   1]\n",
      " [  7  15   7]\n",
      " [  5  23 238]]\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Data Training =====\")\n",
    "for label in ['negatif', 'netral', 'positif']:\n",
    "    print(f\"{label}: {sum(train_df['Sentimen'] == label)}\")\n",
    "print()\n",
    "\n",
    "print(\"===== Data Testing =====\")\n",
    "for label in ['negatif', 'netral', 'positif']:\n",
    "    print(f\"{label}: {sum(test_df['Sentimen'] == label)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Akurasi tanpa menggunakan SMOTE: {accuracy_score(test_df['Sentimen'], y_pred):.2f}\")\n",
    "\n",
    "kelas = ['negatif', 'netral', 'positif']\n",
    "presisi = precision_score(test_df['Sentimen'], y_pred, labels=kelas, average=None, zero_division=0)\n",
    "recall = recall_score(test_df['Sentimen'], y_pred, labels=kelas, average=None, zero_division=0)\n",
    "for i, label in enumerate(kelas):\n",
    "    print(f\"Kelas {label}: Presisi = {presisi[i]:.2f}, Recall = {recall[i]:.2f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix tanpa menggunakan SMOTE:\\n{confusion_matrix(test_df['Sentimen'], y_pred)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Akurasi menggunakan SMOTE: {accuracy_score(test_df['Sentimen'], y_pred_smote):.2f}\")\n",
    "\n",
    "kelas = ['negatif', 'netral', 'positif']\n",
    "presisi_smote = precision_score(test_df['Sentimen'], y_pred_smote, labels=kelas, average=None, zero_division=0)\n",
    "recall_smote = recall_score(test_df['Sentimen'], y_pred_smote, labels=kelas, average=None, zero_division=0)\n",
    "for i, label in enumerate(kelas):\n",
    "    print(f\"Kelas {label} setelah SMOTE: Presisi = {presisi_smote[i]:.2f}, Recall = {recall_smote[i]:.2f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix menggunakan SMOTE:\\n{confusion_matrix(test_df['Sentimen'], y_pred_smote)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
